BASE_TASK_CONFIG_PATH: "configs/tasks/objectnav_mp3d_il_rl_ft_robot_settings_with_noise.yaml"
CMD_TRAILING_OPTS: ["TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS", "50000"]
ENV_NAME: "ObjectNavRLEnv"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: []
TENSORBOARD_DIR: "tb/objectnav_il_rl_ft/overfitting/seed_1/"
VIDEO_DIR: "video_dir"
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "data/new_checkpoints/objectnav_il_rl_ft/overfitting/seed_1/"
NUM_PROCESSES: 8
CHECKPOINT_FOLDER: "data/new_checkpoints/objectnav_il_rl_ft/overfitting/seed_1/"
TRAINER_NAME: "ddppo"
SENSORS: ["DEPTH_SENSOR", "RGB_SENSOR"]
NUM_UPDATES: 50000
LOG_INTERVAL: 10
CHECKPOINT_INTERVAL: 500
EVAL:
  SPLIT: "val"
  meta_file: "tb/objectnav_il_rl_ft/overfitting/seed_1/evaluation_meta.json"
WANDB:
  GROUP_NAME: "rgbd"
  PROJECT_NAME: "objectnav_il_rl"
  JOB_TYPE: "train"
  MODE: "online"
  RESUME: "must"
  TAGS: ["sparse_reward", "mlp_critic", "lr_decay"]
  LOG_DIR: "wandb/"

RL:
  SUCCESS_REWARD: 2.5
  SLACK_REWARD: -1e-3

  POLICY:
    name: "ObjectNavILPolicy"

  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 2
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 64
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 2048

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    distrib_backend: NCCL
    # Visual encoder backbone
    pretrained_weights: /srv/flash1/rramrakhya6/habitat-web/habitat-lab/data/new_checkpoints/objectnav/objectnav_hm3d_hd_20k_ft/sem_seg_pred/seed_2/ckpt.22.pth
    # Initialize with pretrained weights
    pretrained: False
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: False
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: True
    # Whether or not to reset the critic linear layer
    reset_critic: True
    downsample_critic: 1

    # Model parameters
    backbone: resnet50
    rnn_type: LSTM
    num_recurrent_layers: 2

  Finetune:
    finetune: True
    freeze_encoders: True

    policy_ft_lr: 1.5e-5

    start_actor_finetuning_at: 50
    actor_lr_warmup_update: 100
    start_critic_warmup_at: 50
    critic_lr_decay_update: 100


MODEL:
  hm3d_goal: False
  embed_sge: True
  USE_SEMANTICS: True
  USE_PRED_SEMANTICS: True
  SWITCH_TO_PRED_SEMANTICS_UPDATE: 0
  SEMANTIC_PREDICTOR:
    name: "coco_maskrcnn"
    REDNET:
      pretrained_weights: "data/rednet-models/rednet_semmap_mp3d_40_v2_vince.pth"
      # pretrained_weights: "data/rednet-models/rednet_semmap_hm3d_6.pth"
      num_classes: 29
    DETIC:
      pretrained_weights: "Detic/models/Detic_LbaseI_CLIP_R5021k_640b64_4x_ft4x_max-size.pth"
      config: "Detic/configs/Detic_LbaseI_CLIP_R5021k_640b64_4x_ft4x_max-size.yaml"
      score_threshold: 0.3
    COCO_MASKRCNN:
      semantic_categories: only_goal
  SEMANTIC_ENCODER:
    cnn_type: "ResnetSemSegEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
    embedding_size: 4
    is_hm3d: False
    is_thda: True
  RGB_ENCODER:
    cnn_type: "ResnetRGBEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
    normalize_visual_inputs: False
  DEPTH_ENCODER:
    cnn_type: "VlnResnetDepthEncoder"
    output_size: 128
    backbone: "resnet50"
    trainable: False
    ddppo_checkpoint: "data/ddppo-models/gibson-2plus-resnet50.pth"
  STATE_ENCODER:
    hidden_size: 2048
    rnn_type: "GRU"
    num_recurrent_layers: 2
  SEQ2SEQ:
    use_prev_action: True
  CRITIC:
    no_critic: False
    mlp_critic: True
    hidden_dim: 512